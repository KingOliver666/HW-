---
title: "STATS 506 HW4"
format: html
editor: visual
embed-resources: true
---

## Problem 1

Github: <https://github.com/KingOliver666/HW-.git>

```{r}
library(tidyverse)
airline <- nycflights13::airlines
airport <- nycflights13::airports
flight <- nycflights13::flights
plane <- nycflights13::planes
weather <- nycflights13::weather
```

a.  Generate a table (which can just be a nicely printed tibble) reporting the mean and median departure delay per airport. Generate a second table (which again can be a nicely printed tibble) reporting the mean and median arrival delay per airport. Exclude any destination with under 10 flights. Do this exclusion through code, not manually.

    Additionally,

    -   Order both tables in descending mean delay.

    -   Both tables should use the airport *names* not the airport *codes*.

    -   Both tables should print all rows.

We display the first table of departure delay:

```{r}
fliport_delay <- flight %>% 
  left_join(airport, by = c("origin" = "faa")) %>% 
  group_by(name) %>% 
  summarize(
    mean_de_delay = mean(dep_delay, na.rm = TRUE),            median_de_delay = median(dep_delay, na.rm = TRUE)
    ) %>% 
  arrange(desc(mean_de_delay)) %>%
  select(name, mean_de_delay, median_de_delay)
  
fliport_delay
```

We then display the second table of arrival delay(That exclude the airport named NA):

```{r}
fliport_arrive <- flight %>%
  left_join(airport, by = c("dest" = "faa")) %>%
  group_by(name) %>%
  summarize(
    mean_ar = mean(arr_delay, na.rm = TRUE),
    median_ar = median(arr_delay, na.rm = TRUE),
    count_dep = n()
  ) %>%
  filter(count_dep >= 10) %>%
  arrange(desc(mean_ar)) %>%
  select(name, mean_ar, median_ar)

fliport_arrive
```

This is the table that include the airport named "NA":

```{r}
fliport_arrive <- flight %>%
  group_by(dest) %>%
  summarize(
    mean_ar = mean(arr_delay, na.rm = TRUE),
    median_ar = median(arr_delay, na.rm = TRUE),
    count_dep = n()
  ) %>%
  filter(count_dep >= 10) %>%
  arrange(desc(mean_ar)) %>%
  left_join(airport, by = c("dest" = "faa")) %>%
  select(name, mean_ar, median_ar)

fliport_arrive
```

b\. How many flights did the aircraft model with the fastest average speed take? Produce a tibble with 1 row, and entires for the model, average speed (in MPH) and number of flights.

```{r}
fast_air <- flight %>% 
  left_join(plane, by = "tailnum" ) %>% 
  group_by(model) %>% 
  summarize(
    total_dis = sum(distance, na.rm = TRUE),
    total_time = sum(air_time, na.rm = TRUE),
    num_flight = n()
  ) %>% 
  mutate(speed = total_dis / (total_time / 60)) %>% 
  arrange(desc(speed)) %>% 
  select(model, speed, num_flight)
  
max_aircraft <- fast_air %>% 
  head(1)
max_aircraft
```

So the aircarft 777-232 has the highest speed with 681mph with 4 flights.

## Problem 2

Use the **tidyverse** for this problem. In particular, use piping and **dplyr** as much as you are able. **Note**: Use of any deprecated functions will result in a point loss.

Load the Chicago NNMAPS data we used in the visualization lectures. Write a function `get_temp()` that allows a user to request the average temperature for a given month. The arguments should be:

-   `month`: Month, either a numeric 1-12 or a string.

-   `year`: A numeric year.

-   `data`: The data set to obtain data from.

-   `celsius`: Logically indicating whther the results should be in celsius. Default `FALSE`.

-   `average_fn`: A function with which to compute the mean. Default is `mean`.

The output should be a numeric vector of length 1. The code inside the function should, as with the rest of this problem, use the **tidyverse**. Be sure to sanitize the input

```{r}
setwd("C:/Users/Lenovo/Downloads")
nnmaps <- read.csv("chicago-nmmaps.csv")
```

```{r}
get_temp <- function(month_input, year_input, data, celsius = FALSE, average_fn = mean) {
  month_vec <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
  month_vec1 <- c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")
  
  ## Here we want to exclude cases with invalid year and month
  if (is.numeric(month_input) && month_input >= 1 && month_input <= 12 && month_input == as.integer(month_input) && year_input >= 1997 && year_input <= 2000) {
    month_input <- month_vec[month_input]
  } else if (substr(month_input, 1, 3) %in% month_vec || month_input %in% month_vec1 && year_input >= 1997 && year_input<= 2000) {
    month_input <- substr(month_input, 1, 3)
  } else {
    stop("Invalid month or year input, please check your input.")
  }
  
  ## get the average temperature in a certain month
  avg_temp <- data %>%
    filter(month_input == month & year_input == year) %>%
    summarize(average_temp = average_fn(temp))
  ## Here we will consider the two kind of measure of temperature
  if (celsius) {
    avg_temp <- avg_temp %>%
      mutate(average_temp = (average_temp - 32) * 5/9)
  }
  
  return(avg_temp$average_temp)
}
```

```{r}
get_temp("Apr", 1999, data = nnmaps)
get_temp("Apr", 1999, data = nnmaps, celsius = TRUE)
get_temp(10, 1998, data = nnmaps, average_fn = median)
get_temp("November", 1999, data =nnmaps, celsius = TRUE,
         average_fn = function(x) {
           x %>% sort -> x
           x[2:(length(x) - 1)] %>% mean %>% return
         })
```

```{r}
## for these two cases, we will get an error. The first one has invalid month and the second one has invalid year

## get_temp(13, 1998, data = nnmaps) 
## get_temp(2, 2005, data = nnmaps)
```

## Problem 3

(a):

```         
%let in_path = ~/sasuser.v94/homework/;
RUN;
FILENAME REFFILE '/home/u63650570/sasuser.v94/homework/recs2020_public_v5.csv';
PROC IMPORT DATAFILE=REFFILE
    DBMS=CSV
    OUT=recs;
    GETNAMES=YES;
RUN;




/*Group our data using state_name and sum the nweight*/
PROC SUMMARY DATA = recs;
 CLASS state_name;
 OUTPUT OUT = state_grouped
 SUM(nweight) = sum_weight;
RUN;

/*Remove the top row*/
DATA recs_grouped;
 SET state_grouped;
 WHERE _type_ = 1;
RUN;

/*Produce a table with descending pertentage of record*/
PROC SQL;
 SELECT state_name, sum_weight / SUM(sum_weight) AS percentage
 FROM recs_grouped
 ORDER BY percentage DESC;
QUIT;
```

Answer: Answer: Thus, California has the highest percentage(10.6%) of records, and Michigan has about 3.17 percent(3.17%) of all records by looking at the table.

(b):

```         
DATA elec;
 SET recs;
 WHERE DOLLAREL > 0;
 KEEP DOLLAREL;
RUN;



PROC UNIVARIATE DATA = elec;
 HISTOGRAM DOLLAREL;
RUN;
```

Answer: Thus, we get a histogram as we desired

(c):

```         
DATA logelec;
/*Do log transformation*/
 SET elec;
 LOGDOLLAREL = log(DOLLAREL);
RUN;

/* Draw the histogram for log cost of electricity*/
PROC UNIVARIATE DATA = logelec;
 HISTOGRAM LOGDOLLAREL;
RUN;
```

Answer: hence, we get the histogram we desired

(d):

```         
/* We only focus on those observations that have positive cost*/

/* We first get those observations that garage = 0 or garage = 1*/
DATA elec_gara;
 SET recs;
 WHERE DOLLAREL > 0 AND PRKGPLC1 >= 0;
 KEEP DOLLAREL PRKGPLC1 TOTROOMS NWEIGHT;
RUN;

DATA logelec_gara;
 SET elec_gara;
 log_DOLLAREL = log(DOLLAREL);
RUN;

PROC REG DATA = logelec_gara;
 MODEL log_DOLLAREL = TOTROOMS PRKGPLC1;
RUN;
```

Answer: we then get the linear model we want

(e):

```         
/* Run the linear regression model */
PROC REG DATA = logelec_gara; 
 MODEL log_DOLLAREL = TOTROOMS PRKGPLC1; 
 OUTPUT OUT=fitted_dataset PREDICTED=fitted_values;
RUN;

/* find exponentials of those fitted values */
DATA exp_gara;
 SET fitted_dataset;
 fitted_value = exp(fitted_values);
RUN;
 
/* Draw the scatterplot*/
PROC SGPLOT DATA = exp_gara;
 SCATTER x=DOLLAREL y=fitted_value;
RUN;
```

Answer: we will then get our desired histogram.

## Problem 4

(b): I used the below code to get the sub-data:

```         
%let in_path = ~/sasuser.v94/homework/;
%let out_path = ~/sasuser.v94/homework/;
libname mylib "&out_path.";
RUN;

FILENAME REFFILE "&in_path/public2022.csv";

/* Import the CSV file */
PROC IMPORT DATAFILE=REFFILE
    DBMS=CSV
    OUT=public2022
    REPLACE;
    GETNAMES=YES;
RUN;

/* Create a subset dataset */
PROC SQL;
CREATE TABLE public2022_filter AS
SELECT CaseID, B3, ND2, B7_b, GH1, ppeducat, race_5cat, weight, weight_pop
FROM public2022;
QUIT;


/* Thus, we got the CSV file we want, and then I will import it into STATA*/
```

(c): I directly downloaded my filtered data from library and then import the CSV to STATA.

```         
import delimited "C:\Users\Lenovo\Downloads\PUBLIC_FILTER.csv", varnames(1)
```

(d): Here we see that our subset of the original data has number of rows of 11667 and totally 9 variables that we want. The number of rows of the subdata matches the number of rows of original data by looking at the codebook.

```         
describe

// Contains data
//  Observations:        11,667                  
//     Variables:             9                  
// -----------------------------------------------------------------------------------
// Variable      Storage   Display    Value
//     name         type    format    label      Variable label
// -----------------------------------------------------------------------------------
// caseid          int     %8.0g                 CaseID
// b3              str19   %19s                  B3
// nd2             str15   %15s                  ND2
// b7_b            str9    %9s                   B7_b
// gh1             str57   %57s                  GH1
// ppeducat        str64   %64s                  
// race_5cat       str8    %9s                   
// weight          float   %9.0g                 
// weight_pop      float   %9.0g                 
// -----------------------------------------------------------------------------------
// Sorted by: 
//      Note: Dataset has changed since last saved.
```

(e):

```         
preserve
replace b3 = "0" if b3 == "Much worse off"
replace b3 = "0" if b3 == "Somewhat worse off"
replace b3 = "1" if b3 == "About the same"
replace b3 = "1" if b3 == "Somewhat better off"
replace b3 = "1" if b3 == "Much better off"

** Change the b3(string type) into numeric variable and rename it as b3_new**
destring b3, generate(b3_new)
```

Answer: Then we have convert the Likert scale into binary variable.

(f):

```         
svyset caseid [pw = weight_pop]

// Sampling weights: weight_pop
//              VCE: linearized
//      Single unit: missing
//         Strata 1: <one>
//  Sampling unit 1: caseid
//            FPC 1: <zero>

** Now I have tell stata that the data is from a complex sample, and then I will carry out a logistic regression. I first need to transform strings in each variable to be numeric
replace nd2 = "1" if nd2 == "Much higher"
replace nd2 = "2" if nd2 == "Somewhat higher"
replace nd2 = "3" if nd2 == "About the same"
replace nd2 = "4" if nd2 == "Somewhat lower"
replace nd2 = "5" if nd2 == "Much lower"
destring nd2, generate(nd2_new)

replace b7_b = "1" if b7_b == "Poor"
replace b7_b = "2" if b7_b == "Only fair"
replace b7_b = "3" if b7_b == "Good"
replace b7_b = "4" if b7_b == "Excellent"
destring b7_b, generate(b7_b_new)

replace gh1 = "1" if gh1 == "Own your home with a mortgage or loan"
replace gh1 = "2" if gh1 == "Own your home free and clear (without a mortgage or loan)"
replace gh1 = "3" if gh1 == "Pay rent"
replace gh1 = "4" if gh1 == "Neither own nor pay rent"
destring gh1, generate(gh1_new)

replace ppeducat = "1" if ppeducat == "No high school diploma or GED"
replace ppeducat = "2" if ppeducat == "High school graduate (high school diploma or the equivalent GED)"
replace ppeducat = "3" if ppeducat == "Some college or Associate's degree"
replace ppeducat = "4" if ppeducat == "Bachelor's degree or higher"
destring ppeducat, generate(ppeducat_new)

replace race_5cat = "1" if race_5cat == "White"
replace race_5cat = "2" if race_5cat == "Black"
replace race_5cat = "3" if race_5cat == "Hispanic"
replace race_5cat = "4" if race_5cat == "Asian"
replace race_5cat = "5" if race_5cat == "Other"
destring race_5cat, generate(race_5cat_new)

** Now I can carry out the logistic regression
svy : logit b3_new i.nd2_new i.b7_b_new i.gh1_new i.ppeducat_new i.race_5cat_new


Survey: Logistic regression

Number of strata =      1                        Number of obs   =      11,667
Number of PSUs   = 11,667                        Population size = 255,114,223
                                                 Design df       =      11,666
                                                 F(17, 11650)    =       56.70
                                                 Prob > F        =      0.0000

-------------------------------------------------------------------------------
              |             Linearized
       b3_new | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
--------------+----------------------------------------------------------------
      nd2_new |
           2  |   .0816722   .0925755     0.88   0.378    -.0997913    .2631356
           3  |   .0618535   .0854686     0.72   0.469    -.1056792    .2293863
           4  |   .2533888   .2045978     1.24   0.216    -.1476572    .6544347
           5  |    .229354   .1672799     1.37   0.170    -.0985426    .5572505
              |
     b7_b_new |
           2  |   1.110649   .0488662    22.73   0.000     1.014863    1.206435
           3  |   1.806251   .0796863    22.67   0.000     1.650052    1.962449
           4  |   2.485125   .3463415     7.18   0.000     1.806238    3.164013
              |
      gh1_new |
           2  |  -.0702921    .056382    -1.25   0.213    -.1808102     .040226
           3  |   .0190607   .0587346     0.32   0.746    -.0960689    .1341904
           4  |   .3465325   .0994184     3.49   0.000     .1516557    .5414092
              |
 ppeducat_new |
           2  |   .0767668   .1036364     0.74   0.459    -.1263778    .2799115
           3  |   .1075004   .1008067     1.07   0.286    -.0900975    .3050983
           4  |   .2288346    .099574     2.30   0.022     .0336528    .4240164
              |
race_5cat_new |
           2  |   .7060141   .0810818     8.71   0.000     .5470803     .864948
           3  |   .1635498   .0711263     2.30   0.021     .0241303    .3029693
           4  |   .4567994   .1259942     3.63   0.000     .2098298    .7037691
           5  |  -.0210142   .1659436    -0.13   0.899    -.3462915    .3042631
              |
        _cons |  -.4852955   .1301287    -3.73   0.000    -.7403696   -.2302214

```

Answer: after the logistic regression, we fail to reject H_0 and we may conclude that the factor nd2_new(thinking that the chance of experiencing a natural disaster or severe weather event will be higher, lower or about the same in 5 years.) is not statistical significant by looking at those P-values, and we may remove this predictor

(g): I directly downloaded the data after use command browser and then saved the data with data form of dta, and then I upload the data to R.

```{r}
library(haven)
data_filter <- read_dta("C:/Users/Lenovo/Downloads/Filter_data.dta")

```

(h):

```{r}
library(survey)
library(pscl)
svydesign(id = ~ caseid, weight = ~ weight_pop, data = data_filter)# set up the complex survey design

logis_model <- glm(b3_new ~ nd2 + b7_b + gh1 + ppeducat + race_5cat, family = binomial, data = data_filter)

pseudo_data_filter <- pR2(logis_model, which = "mcfadden")
pseudo_data_filter
```

Thus, we conclude that the Pseudo R\^2 is 0.0928.
